{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["oIN3TiPxuhea","LpXHKkakultS","pDxGlM9YvA1J","ln29il4542cC","mrUMvd9yvmXy","Xx2VDvNh0kjB"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Connect\n","Connect to Google Drive and ensure dataset folder is present"],"metadata":{"id":"oIN3TiPxuhea"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OM2tZBwYtUlE","executionInfo":{"status":"ok","timestamp":1683275161372,"user_tz":-480,"elapsed":25680,"user":{"displayName":"Matthew Antoni De Mesa","userId":"14499233318177830542"}},"outputId":"7b3c8e0e-f0d9-4b0b-9ef7-269f3bb6d00f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/Shareddrives/ECE 199 2s2223/Dataset\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","%cd \"/content/gdrive/Shareddrives/ECE 199 2s2223/Dataset/\""]},{"cell_type":"markdown","source":["## Imports\n"],"metadata":{"id":"LpXHKkakultS"}},{"cell_type":"code","source":["from json import dump, load, loads\n","from collections import Counter\n","from itertools import chain\n","from numpy.random import choice, seed\n","from math import log10\n","from itertools import product\n","from os import listdir\n","from os.path import join"],"metadata":{"id":"a9n0QM4QumsB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inputs\n","This is the only part that must be changed between runs. Values here are the default."],"metadata":{"id":"pDxGlM9YvA1J"}},{"cell_type":"code","source":["# Dataset Constants\n","PARTICIPANT_COUNT = 20\n","VIDEO_PER_PARTICIPANT = 50\n","\n","# Arbitrary but fixed number for random.shuffle\n","seed_number = 1234\n","\n","# Total fixed number of samples dedicated for training\n","train_size = 800 \n","\n","# Main source of annotation. Entire code is very format dependent\n","dataset_path = \"/content/gdrive/Shareddrives/ECE 199 2s2223/Dataset\"\n","\n","## Dataset Subdirectories (in Google Drive)\n","depth_dataset_path = f\"{dataset_path}/Final/DEPTHS\"\n","video_dataset_path = f\"{dataset_path}/Final/VIDEOS\"\n","\n","# Video-Intent-Gaze JSON source\n","label_filepath = f\"{dataset_path}/intent_ann_new.json\"\n","\n","# Output Filepath\n","save_filepath = f\"{dataset_path}/Splits/gaze_dataset.json\""],"metadata":{"id":"oZ-_d4wUvBM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Process\n","Prerequisite functions that describe the entire splitting process."],"metadata":{"id":"TM5v185Wuwjx"}},{"cell_type":"markdown","source":["### Filepath Functions"],"metadata":{"id":"ln29il4542cC"}},{"cell_type":"code","source":["def participant_directories(format=\"\", root=\"\"):\n","    video_range = range(1, VIDEO_PER_PARTICIPANT + 1)\n","    participant_range = range(1, PARTICIPANT_COUNT + 1)\n","\n","    for participant, video in product(participant_range, video_range):\n","        directory =  join(\n","            root, \n","            f\"P{participant}/V{video}\", \n","            f\"P{participant}_V{video}.{format}\" if format else \"\")\n","        yield directory, participant, video"],"metadata":{"id":"KICkiAXS46OS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def filename(root, participant, video, format=\"\"):\n","    return f\"{root}/P{participant}/V{video}/P{participant}_V{video}.{format}\""],"metadata":{"id":"gfN9WoWQwwk_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def format_filename(root, filepath):\n","    filename = filepath.split('.')[0]\n","    participant, video = filename.split('_')[:2]\n","    participant, video =int( participant[1:]), int(video[1:])\n","\n","    return f\"{root}/P{participant}/V{video}/{filepath}\""],"metadata":{"id":"SIQxf1KEJx8V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Depth Inclusion"],"metadata":{"id":"mrUMvd9yvmXy"}},{"cell_type":"code","source":["def depth_files(directory):\n","    return [format_filename(\"DEPTHS\", path) for path in listdir(directory)]"],"metadata":{"id":"DvNOS-ZMxwzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def video_depth_mapping():\n","    directories = participant_directories(\n","        root=depth_dataset_path)\n","\n","    return { filename(\"VIDEOS\", p, v, 'mp4'): depth_files(d)\n","            for d, p, v in directories }"],"metadata":{"id":"E5sm_dwSwzna"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bounding Box Inclusion"],"metadata":{"id":"Xx2VDvNh0kjB"}},{"cell_type":"code","source":["def box_format(item):\n","    return [\n","        int(item[\"classifications\"] != []),\n","        item[\"bbox\"][\"top\"], \n","        item[\"bbox\"][\"left\"], \n","        item[\"bbox\"][\"width\"], \n","        item[\"bbox\"][\"height\"]\n","    ]"],"metadata":{"id":"X5sdI8FsCB2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def bounding_boxes(label_filepath):\n","    with open(label_filepath) as labels:\n","        frames = [loads(label) for label in labels]\n","\n","    return [ { item[\"title\"]: box_format(item) \n","        for item in frame[\"objects\"] }\n","        for frame in frames ]"],"metadata":{"id":"DzLr_YZ297Cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def video_label_mapping():\n","    directories = participant_directories(\n","        root=video_dataset_path,\n","        format=\"ndjson\")\n","\n","    return {filename(\"VIDEOS\", p, v, 'mp4'): bounding_boxes(d) \n","        for d, p, v in directories}"],"metadata":{"id":"U8h3QcQc2lml"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Splitting Mechanics"],"metadata":{"id":"qmxVo_WA0a8w"}},{"cell_type":"code","source":["def weighted_object_dataset(label_filepath):\n","    with open(label_filepath) as dataset:\n","        dataset = load(dataset)\n","\n","    print(\"Loading Gaze Dataset\")\n","\n","    # Reformat video mapping into list of objects\n","    labels = dataset[\"hiphop\"][\"videos\"]\n","\n","    # Maps the list of PNG depth filepaths for a given video\n","    video_depth_map = video_depth_mapping()\n","    video_label_map = video_label_mapping()\n","\n","    # Sample Format\n","    samples = [\n","        {\n","            \"video\": format_filename(\"VIDEOS\", video),\n","            \"gaze_seq\": label[\"gaze_seq\"],\n","            \"bbox\": video_label_map[format_filename(\"VIDEOS\", video)],\n","            \"depth\": video_depth_map[format_filename(\"VIDEOS\", video)],\n","        }\n","        for video, label in labels.items()\n","    ]\n","\n","    return samples"],"metadata":{"id":"A6RhhKBvuyNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def low_frequency_weighting(samples):\n","    print(\"Calculating weights\")\n","\n","    gaze_sequences = [sample[\"gaze_seq\"] for sample in samples]\n","    object_counter = Counter(chain.from_iterable(gaze_sequences))\n","    total = sum(1 / log10(count) for count in object_counter.values())\n","\n","    weight = {\n","        key: (1 / log10(count)) / (total * object_counter[key])\n","        for key, count in object_counter.items()\n","    }\n","    weights = [sum(weight[x] for x in gaze) for gaze in gaze_sequences]\n","\n","    return weights"],"metadata":{"id":"8FWE9UxeuzmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split(samples, seed_number, train_size):\n","    seed(seed_number)\n","\n","    print(f\"Splitting Label Dataset (seed={seed_number})\")\n","\n","    weights = low_frequency_weighting(samples)\n","    \n","    training_set = choice(samples, size=train_size, p=weights, replace=False)\n","    training_set = list(training_set)\n","\n","    testing_set = [sample for sample in samples if sample not in training_set]\n","\n","    train_gazes = [sample[\"gaze_seq\"] for sample in training_set]\n","    test_gazes = [sample[\"gaze_seq\"] for sample in testing_set]\n","\n","    train_counter = Counter(chain.from_iterable(train_gazes))\n","    test_counter = Counter(chain.from_iterable(test_gazes))\n","\n","    total = sum(train_counter.values()) + sum(test_counter.values())\n","\n","    print(f\"Train | Test Split ({len(training_set)} | {len(testing_set)})\")\n","\n","    for object in train_counter:\n","        train_percent = train_counter[object]/total\n","        test_percent = test_counter[object]/total\n","        print(\n","            f\"\\t{train_percent:.5%}\\t|  {test_percent:.5%}\\t | {object}\"\n","        )\n","\n","    return training_set, testing_set"],"metadata":{"id":"f-aKtRVLu2HB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_gaze_dataset(training_set, testing_set, save_path):\n","    # HIPHOP Gaze Dataset Format\n","    dataset = {\n","        \"hiphop\": {\n","            \"gaze\": {\n","                \"train\": training_set,\n","                \"test\": testing_set,\n","            }\n","        }\n","    }\n","\n","    # Save dataset\n","    with open(save_path, \"w\") as gaze_dataset:\n","        dump(dataset, gaze_dataset)"],"metadata":{"id":"7miq7NZpu45D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate\n","\n","Run cell to generate a gaze split in json format. Note that any existing gaze split will be overwritten."],"metadata":{"id":"g6FytmGJu6jY"}},{"cell_type":"code","source":["samples = weighted_object_dataset(label_filepath)\n","training_set, testing_set = split(samples, seed_number, train_size)\n","save_gaze_dataset(training_set, testing_set, save_filepath)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RiC7_c2ju7CL","executionInfo":{"status":"ok","timestamp":1683277112480,"user_tz":-480,"elapsed":1951116,"user":{"displayName":"Matthew Antoni De Mesa","userId":"14499233318177830542"}},"outputId":"31d4666b-2b86-4dcd-c8a6-f2f5e0a655f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Gaze Dataset\n","Splitting Label Dataset (seed=1234)\n","Calculating weights\n","Train | Test Split (800 | 200)\n","\t6.12351%\t|  1.58252%\t | Broom\n","\t6.90408%\t|  2.41266%\t | Pillow\n","\t6.84624%\t|  1.08579%\t | Book\n","\t3.78132%\t|  0.40146%\t | Cup\n","\t4.79373%\t|  0.58081%\t | Laptop\n","\t5.88049%\t|  0.47485%\t | Fruits\n","\t6.69994%\t|  1.34388%\t | Bottle\n","\t12.41567%\t|  8.68198%\t | none\n","\t3.01145%\t|  0.12248%\t | Sandwich\n","\t4.32179%\t|  0.66246%\t | Rug\n","\t5.53833%\t|  0.53366%\t | Racket\n","\t4.52252%\t|  1.02796%\t | Umbrella\n","\t2.84960%\t|  0.31057%\t | Bag\n","\t2.81995%\t|  0.14727%\t | Bowl\n","\t1.85956%\t|  0.03791%\t | Utensils\n","\t2.14534%\t|  0.08020%\t | Chair\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"YKCGM9XFZs9c"},"execution_count":null,"outputs":[]}]}