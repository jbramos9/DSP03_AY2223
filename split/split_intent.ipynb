{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"BExoZkHIuZV6"},"source":["# Intention Dataset Split\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mlYBJZzsqLgt"},"source":["## Imports\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1686580433744,"user":{"displayName":"Stephen Singer","userId":"07833907350573086053"},"user_tz":-480},"id":"4AGhLgiiqI4G"},"outputs":[],"source":["from json import load, dump\n","from collections import defaultdict\n","from random import shuffle, seed"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"i1WPZigPtrXc"},"source":["## Inputs\n","\n","This is the only part that must be changed between runs. Values here are the default."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DYhrU_6rql-k"},"outputs":[],"source":["# Arbitrary but fixed number for random.shuffle\n","seed_number = 1234\n","\n","# Per Intention (100 samples * 8 intentions = 800 training samples)\n","train_size = 100 \n","\n","# Main source of annotation. Entire code is very format dependent\n","dataset_path = \"../dataset\"\n","video_dataset_path = f\"{dataset_path}/VIDEOS\"\n","\n","\n","label_filepath = f\"./intent_ann_new.json\"\n","save_filepath = f\"./data/intent_dataset.json\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pI7fEtTDt1V6"},"source":["## Process\n","Prerequisite functions that describe the entire splitting process."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"GBgKYvtDqPF8"},"outputs":[],"source":["def intention_sorted_gazeseq(label_filepath):    \n","    with open(label_filepath) as dataset:\n","        dataset = load(dataset)\n","        labels = dataset[\"hiphop\"][\"videos\"]\n","\n","    # Group samples into a dictionary with intentions as keys and values \n","    # as list of samples with said intention\n","    samples = defaultdict(list)\n","\n","    for _, label in labels.items():\n","        samples[label[\"intent\"]].append(label[\"gaze_seq\"])\n","\n","    print(\"Grouping Label Dataset by Intention\")\n","\n","    label_items = sum(len(sequence) for sequence in samples.values())\n","\n","    print(f\"Label Set ({label_items} items)\")\n","    for intent, sequences in samples.items():\n","        items = len(sequences)\n","        print(f\"\\t{items} items ({items/label_items:2.2%}):\\t{intent}\")\n","\n","    return samples"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"sIS1tj48qXwl"},"outputs":[],"source":["def split(samples, seed_number, train_size):\n","    seed(seed_number)\n","\n","    training_set = defaultdict(list)\n","    testing_set = defaultdict(list)\n","\n","    for intent in samples:\n","        shuffle(samples[intent])\n","        training_set[intent] = samples[intent][:train_size]\n","        testing_set[intent] = samples[intent][train_size:]\n","\n","    print(f\"Splitting Label Dataset (seed={seed_number})\")\n","\n","    train_items = sum(len(sequence) for sequence in training_set.values())\n","    test_items = sum(len(sequence) for sequence in testing_set.values())\n","\n","    print(f\"Training Set ({train_items} items)\")\n","    for intent, sequences in training_set.items():\n","        items = len(sequences)\n","        print(f\"\\t{items} items ({items/train_items:2.2%}):\\t{intent}\")\n","\n","    print(f\"Testing Set ({test_items} items)\")\n","    for intent, sequences in testing_set.items():\n","        items = len(sequences)\n","        print(f\"\\t{items} items  ({items/test_items:2.2%}):\\t{intent}\")\n","\n","    return training_set, testing_set"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"xFQv19v-qcL2"},"outputs":[],"source":["def save_intention_dataset(training_set, testing_set, save_filepath):\n","    # HIPHOP Intention Dataset Format\n","    dataset = {\n","        \"hiphop\": {\n","            \"intentions\": {\n","                \"train\": training_set,\n","                \"test\": testing_set,\n","            }\n","        }\n","    }\n","\n","    # Save dataset\n","    with open(save_filepath, \"w\") as intention_dataset:\n","        dump(dataset, intention_dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qc4Y-Yu-uCLZ"},"source":["## Generate\n","Run cell to generate an intention split in json format. Note that any existing intention split will be overwritten."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1272,"status":"ok","timestamp":1681474359333,"user":{"displayName":"Stephen Singer","userId":"07833907350573086053"},"user_tz":-480},"id":"ue8Fr4dmqg6D","outputId":"61e91d05-bfb2-4602-876d-9fda730f089d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Grouping Label Dataset by Intention\n","Label Set (1000 items)\n","\t127 items (12.70%):\tSpontaneous\n","\t120 items (12.00%):\tDrink\n","\t127 items (12.70%):\tStudy\n","\t115 items (11.50%):\tGo Outside\n","\t132 items (13.20%):\tEat\n","\t135 items (13.50%):\tIndeterminate\n","\t124 items (12.40%):\tClean the Area\n","\t120 items (12.00%):\tRest\n","Splitting Label Dataset (seed=1234)\n","Training Set (800 items)\n","\t100 items (12.50%):\tSpontaneous\n","\t100 items (12.50%):\tDrink\n","\t100 items (12.50%):\tStudy\n","\t100 items (12.50%):\tGo Outside\n","\t100 items (12.50%):\tEat\n","\t100 items (12.50%):\tIndeterminate\n","\t100 items (12.50%):\tClean the Area\n","\t100 items (12.50%):\tRest\n","Testing Set (200 items)\n","\t27 items  (13.50%):\tSpontaneous\n","\t20 items  (10.00%):\tDrink\n","\t27 items  (13.50%):\tStudy\n","\t15 items  (7.50%):\tGo Outside\n","\t32 items  (16.00%):\tEat\n","\t35 items  (17.50%):\tIndeterminate\n","\t24 items  (12.00%):\tClean the Area\n","\t20 items  (10.00%):\tRest\n","Finished\n"]}],"source":["samples = intention_sorted_gazeseq(label_filepath)\n","training_set, testing_set = split(samples, seed_number, train_size)\n","save_intention_dataset(training_set, testing_set, save_filepath)\n","\n","print(\"Finished\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1YwWqL8IduZ5"},"source":["## Uniform Split\n","\n","Once the intent and gaze JSON file for the train-test splits are created, the intent dataset can use the same split used by the gaze dataset with the code below. C"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Inputs\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["gaze_dataset_filepath = \"./data/gaze_dataset.json\"\n","save_filepath_uniform = \"./data/intent_dataset_uniform.json\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Generate\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Tm7kkSQd3pZ"},"outputs":[],"source":["def extract(sample):\n","    return sample[\"video\"].split(\"/\")[-1]\n","\n","\n","def main():\n","    # Dynamically load source of\n","    with open(save_filepath) as dataset:\n","        intentions = load(dataset)[\"hiphop\"][\"intentions\"][\"train\"].keys()\n","\n","    # Source of distribution\n","    with open(gaze_dataset_filepath) as dataset:\n","        gaze_dataset = load(dataset)[\"hiphop\"][\"gaze\"]\n","\n","    train_filenames = [extract(sample) for sample in gaze_dataset[\"train\"]]\n","    test_filenames = [extract(sample) for sample in gaze_dataset[\"test\"]]\n","\n","    # Source of gaze sequence\n","    with open(label_filepath) as dataset:\n","        intent_dataset = load(dataset)[\"hiphop\"][\"videos\"]\n","\n","    # Build new intent dataset\n","\n","    # Train Set\n","    train_set = {intention: [] for intention in intentions}\n","    for filename in train_filenames:\n","        gaze = intent_dataset[filename][\"gaze_seq\"]\n","        intention = intent_dataset[filename][\"intent\"]\n","\n","        train_set[intention].append(gaze)\n","\n","    # Test Split\n","    test_set = {intention: [] for intention in intentions}\n","    for filename in test_filenames:\n","        gaze = intent_dataset[filename][\"gaze_seq\"]\n","        intention = intent_dataset[filename][\"intent\"]\n","\n","        test_set[intention].append(gaze)\n","\n","    unified_dataset = {\n","        \"hiphop\": {\"intentions\": {\"train\": train_set, \"test\": test_set}}\n","    }\n","\n","    with open(save_filepath_uniform, \"w\") as intent_dataset_file:\n","        dump(unified_dataset, intent_dataset_file)\n","\n","main()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNRL71AIpScwiNygcFbGVCy","collapsed_sections":["cVyzFZRpqxPW","mlYBJZzsqLgt","i1WPZigPtrXc","qc4Y-Yu-uCLZ"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
